{
  
    
        "post0": {
            "title": "Assignment 3",
            "content": "The plotting library I choose is Altair. The reason is that: . The interactive plot created by ipywidgets cannot be displayed in the fastpage, as is shown in the forum: link here and it is also not easy to put plotly figures in the fastpage blog. . | Altair also offers a variety of interactive options with sliders and dropdowns, which can make the plot more vivid etc. . | . import pandas as pd import altair as alt from vega_datasets import data . Task 1 . The first dataset is about the malaria deaths by country for all ages across the world and time. The entity is the full country name, the code column is the ISO3166 code. The head of the data is shown below. Considering that the data contains countries and time, the first plot we can consider is the map plot with a time slider. . df = pd.read_csv(&#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-11-13/malaria_deaths.csv&#39;) df.head() . Entity Code Year Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people) . 0 Afghanistan | AFG | 1990 | 6.802930 | . 1 Afghanistan | AFG | 1991 | 6.973494 | . 2 Afghanistan | AFG | 1992 | 6.989882 | . 3 Afghanistan | AFG | 1993 | 7.088983 | . 4 Afghanistan | AFG | 1994 | 7.392472 | . In order to draw a map plot in altair, we need to add the country code to the original dataset so that we can map the countries into the world map. The country_info dataset contains full infomartion about each country, including the FIFA code, ISO3166 code etc. . country_info = pd.read_csv(&quot;https://raw.githubusercontent.com/datasets/country-codes/master/data/country-codes.csv&quot;,dtype = &#39;str&#39;) country_info.head() . FIFA Dial ISO3166-1-Alpha-3 MARC is_independent ISO3166-1-numeric GAUL FIPS WMO ISO3166-1-Alpha-2 ... Sub-region Name official_name_ru Global Name Capital Continent TLD Languages Geoname ID CLDR display name EDGAR . 0 TPE | 886 | TWN | ch | Yes | 158 | 925 | TW | | TW | ... | NaN | NaN | NaN | Taipei | AS | .tw | zh-TW,zh,nan,hak | 1668284 | Taiwan | NaN | . 1 AFG | 93 | AFG | af | Yes | 004 | 1 | AF | AF | AF | ... | Southern Asia | Афганистан | World | Kabul | AS | .af | fa-AF,ps,uz-AF,tk | 1149361 | Afghanistan | B2 | . 2 ALB | 355 | ALB | aa | Yes | 008 | 3 | AL | AB | AL | ... | Southern Europe | Албания | World | Tirana | EU | .al | sq,el | 783754 | Albania | B3 | . 3 ALG | 213 | DZA | ae | Yes | 012 | 4 | AG | AL | DZ | ... | Northern Africa | Алжир | World | Algiers | AF | .dz | ar-DZ | 2589581 | Algeria | B4 | . 4 ASA | 1-684 | ASM | as | Territory of US | 016 | 5 | AQ | | AS | ... | Polynesia | Американское Самоа | World | Pago Pago | OC | .as | en-AS,sm,to | 5880801 | American Samoa | B5 | . 5 rows × 56 columns . We can do some data transformation to the original malaria death dataset and filter out the country that are in the country_info dataset . df.columns = [&#39;name&#39;, &#39;ISO3166-1-Alpha-3&#39;,&#39;Year&#39;,&#39;Death_Rate&#39;] # Change the column name to make the name of two datasets more consistent. df_new = df[df[&#39;ISO3166-1-Alpha-3&#39;].isin(country_info[&#39;ISO3166-1-Alpha-3&#39;])] # check whether the country exists in the country_info dataset df_new.head() . name ISO3166-1-Alpha-3 Year Death_Rate . 0 Afghanistan | AFG | 1990 | 6.802930 | . 1 Afghanistan | AFG | 1991 | 6.973494 | . 2 Afghanistan | AFG | 1992 | 6.989882 | . 3 Afghanistan | AFG | 1993 | 7.088983 | . 4 Afghanistan | AFG | 1994 | 7.392472 | . Merge the two dataset and exclude irrelevant columns . df_final = pd.merge(df_new, country_info , on = &#39;ISO3166-1-Alpha-3&#39;, how = &#39;left&#39;) df_final = df_final[[&#39;name&#39;,&#39;Year&#39;,&#39;Death_Rate&#39;,&#39;ISO3166-1-numeric&#39;]] df_final.head() . name Year Death_Rate ISO3166-1-numeric . 0 Afghanistan | 1990 | 6.802930 | 004 | . 1 Afghanistan | 1991 | 6.973494 | 004 | . 2 Afghanistan | 1992 | 6.989882 | 004 | . 3 Afghanistan | 1993 | 7.088983 | 004 | . 4 Afghanistan | 1994 | 7.392472 | 004 | . df_final.head() . name Year Death_Rate ISO3166-1-numeric . 0 Afghanistan | 1990 | 6.802930 | 004 | . 1 Afghanistan | 1991 | 6.973494 | 004 | . 2 Afghanistan | 1992 | 6.989882 | 004 | . 3 Afghanistan | 1993 | 7.088983 | 004 | . 4 Afghanistan | 1994 | 7.392472 | 004 | . Set the slider . alt.data_transformers.disable_max_rows() #The default row that altair can take is 5000, we need to specify the disable_max_rows if the rows are over 5000 countries = alt.topo_feature(data.world_110m.url, &#39;countries&#39;) # Set the silder, step = 1, min year is 1990, max year is 2016 slider = alt.binding_range( step=1, min=1990, max=2016 ) select_date = alt.selection_single( name=&quot;Slider&quot;, fields=[&#39;Year&#39;], bind=slider, ) . alt.Chart(df_final).mark_geoshape() .encode(color=&#39;Death_Rate:Q&#39;) .add_selection(select_date) .transform_filter(select_date) .transform_lookup( lookup=&#39;ISO3166-1-numeric&#39;, from_=alt.LookupData(countries, key=&#39;id&#39;, fields=[&quot;type&quot;, &quot;properties&quot;, &quot;geometry&quot;]) ) .project(&#39;equirectangular&#39;) .properties( width=400, height=300, title=&#39;Malaria Death Rate (per 100,000 people) &#39; ) . From the above map plot, we can see that most of the countries have death rate less than 50 per 100,000 people, countries with high malaria death rate are more likely to be in Afirica and as time goes by, the death rate continues to decrease. However, one problem with this plot may be that we cannot tell the trend of a single country, it is difficult to distinguish each country. Another plot we can try is the line chart with country as dropdown list. . df_final[&#39;Year&#39;] = pd.to_datetime(df_final[&#39;Year&#39;], format=&#39;%Y&#39;) #Change the year to datetime type country_list = df_final[&#39;name&#39;].dropna().unique() country_list = country_list.tolist() dropdown = alt.binding_select( options = country_list ) select_country = alt.selection_single( name=&quot;dropdown&quot;, fields=[&#39;name&#39;], bind = dropdown, ) alt.Chart(df_final).mark_line().encode( x=&#39;Year&#39;, y=&#39;Death_Rate&#39;, ).add_selection( select_country ).transform_filter( select_country ).properties( width = 500, height=400, title=f&#39;Malaria Death Rate for a singe country (per 100,000 people)&#39; ).configure_axis( grid=False ) . From the line plot, we can see how the death rate of each country change over time. . Task 2 . The second dataset is about the malaria incidence by country for all ages across the world and time. The entity is the full country name, the code column is the ISO3166 code. The head of the data is shown below. . df1 = pd.read_csv(&#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-11-13/malaria_inc.csv&#39;) df1.head() . Entity Code Year Incidence of malaria (per 1,000 population at risk) (per 1,000 population at risk) . 0 Afghanistan | AFG | 2000 | 107.100000 | . 1 Afghanistan | AFG | 2005 | 46.500000 | . 2 Afghanistan | AFG | 2010 | 23.900000 | . 3 Afghanistan | AFG | 2015 | 23.600000 | . 4 Algeria | DZA | 2000 | 0.037746 | . We can do some data transformation to the original malaria incidence dataset and filter out the country that are in the country_info dataset . df1.columns = [&#39;name&#39;, &#39;ISO3166-1-Alpha-3&#39;,&#39;Year&#39;,&#39;Incidence_Rate&#39;] df1_new = df1[df1[&#39;ISO3166-1-Alpha-3&#39;].isin(country_info[&#39;ISO3166-1-Alpha-3&#39;])] df1_new.head() . name ISO3166-1-Alpha-3 Year Incidence_Rate . 0 Afghanistan | AFG | 2000 | 107.100000 | . 1 Afghanistan | AFG | 2005 | 46.500000 | . 2 Afghanistan | AFG | 2010 | 23.900000 | . 3 Afghanistan | AFG | 2015 | 23.600000 | . 4 Algeria | DZA | 2000 | 0.037746 | . df1_final = pd.merge(df1_new, country_info , on = &#39;ISO3166-1-Alpha-3&#39;, how = &#39;left&#39;) df1_final = df1_final[[&#39;name&#39;,&#39;Year&#39;,&#39;Incidence_Rate&#39;,&#39;ISO3166-1-numeric&#39;]] df1_final.head() . name Year Incidence_Rate ISO3166-1-numeric . 0 Afghanistan | 2000 | 107.100000 | 004 | . 1 Afghanistan | 2005 | 46.500000 | 004 | . 2 Afghanistan | 2010 | 23.900000 | 004 | . 3 Afghanistan | 2015 | 23.600000 | 004 | . 4 Algeria | 2000 | 0.037746 | 012 | . alt.data_transformers.disable_max_rows() countries = alt.topo_feature(data.world_110m.url, &#39;countries&#39;) slider1 = alt.binding_range( step=5, min=2000, max=2015 ) select_date1 = alt.selection_single( name=&quot;slider&quot;, fields=[&#39;Year&#39;], bind=slider1, ) . alt.Chart(df1_final).mark_geoshape() .encode(color=&#39;Incidence_Rate:Q&#39;) .add_selection(select_date1) .transform_filter(select_date1) .transform_lookup( lookup=&#39;ISO3166-1-numeric&#39;, from_=alt.LookupData(countries, key=&#39;id&#39;, fields=[&quot;type&quot;, &quot;properties&quot;, &quot;geometry&quot;]) ) .project(&#39;equirectangular&#39;) .properties( width=400, height=300, title=&#39;Malaria Incidence Rate (per 1,000 people) &#39; ) . df1_final[&#39;Year&#39;] = pd.to_datetime(df1_final[&#39;Year&#39;], format=&#39;%Y&#39;) #Change the year to datetime type country_list1 = df1_final[&#39;name&#39;].dropna().unique() country_list1 = country_list1.tolist() dropdown1 = alt.binding_select( options = country_list1 ) select_country1 = alt.selection_single( name=&quot;dropdown&quot;, fields=[&#39;name&#39;], bind = dropdown1, ) alt.Chart(df1_final).mark_line().encode( x=&#39;Year&#39;, y=&#39;Incidence_Rate&#39;, ).add_selection( select_country1 ).transform_filter( select_country1 ).properties( width = 500, height=400, title=f&#39;Malaria Incidence Rate for a singe country (per 1,000 people)&#39; ).configure_axis( grid=False ) . Task 3 . The third dataset is about the malaria deathe by country for all ages across the world and time. The entity is the full country name, the code column is the ISO3166 code. The head of the data is shown below.Malaria deaths by age across the world and time . df2 = pd.read_csv(&#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-11-13/malaria_deaths_age.csv&#39;) df2.head() . Unnamed: 0 entity code year age_group deaths . 0 1 | Afghanistan | AFG | 1990 | Under 5 | 184.606435 | . 1 2 | Afghanistan | AFG | 1991 | Under 5 | 191.658193 | . 2 3 | Afghanistan | AFG | 1992 | Under 5 | 197.140197 | . 3 4 | Afghanistan | AFG | 1993 | Under 5 | 207.357753 | . 4 5 | Afghanistan | AFG | 1994 | Under 5 | 226.209363 | . df2.columns = [&#39;index&#39;,&#39;name&#39;, &#39;ISO3166-1-Alpha-3&#39;,&#39;Year&#39;,&#39;Age_Group&#39;,&#39;Death_Rate&#39;] df2_new = df2[df2[&#39;ISO3166-1-Alpha-3&#39;].isin(country_info[&#39;ISO3166-1-Alpha-3&#39;])] df2_new.head() . index name ISO3166-1-Alpha-3 Year Age_Group Death_Rate . 0 1 | Afghanistan | AFG | 1990 | Under 5 | 184.606435 | . 1 2 | Afghanistan | AFG | 1991 | Under 5 | 191.658193 | . 2 3 | Afghanistan | AFG | 1992 | Under 5 | 197.140197 | . 3 4 | Afghanistan | AFG | 1993 | Under 5 | 207.357753 | . 4 5 | Afghanistan | AFG | 1994 | Under 5 | 226.209363 | . df2_final = pd.merge(df2_new, country_info , on = &#39;ISO3166-1-Alpha-3&#39;, how = &#39;left&#39;) df2_final = df2_final[[&#39;name&#39;,&#39;Year&#39;,&#39;Death_Rate&#39;,&#39;ISO3166-1-numeric&#39;,&#39;Age_Group&#39;]] df2_final.head() . name Year Death_Rate ISO3166-1-numeric Age_Group . 0 Afghanistan | 1990 | 184.606435 | 004 | Under 5 | . 1 Afghanistan | 1991 | 191.658193 | 004 | Under 5 | . 2 Afghanistan | 1992 | 197.140197 | 004 | Under 5 | . 3 Afghanistan | 1993 | 207.357753 | 004 | Under 5 | . 4 Afghanistan | 1994 | 226.209363 | 004 | Under 5 | . The plot I choose is the line plot with dropdown, different color represent different age group. So based on this plot, we can see the trend of death rate of different countries for different age group. . df2_final[&#39;Year&#39;] = pd.to_datetime(df2_final[&#39;Year&#39;], format=&#39;%Y&#39;) country_list2 = df2_final[&#39;name&#39;].dropna().unique() country_list2 = country_list2.tolist() dropdown2 = alt.binding_select( options = country_list2 ) select_country2 = alt.selection_single( name=&quot;dropdown&quot;, fields=[&#39;name&#39;], bind = dropdown2, ) alt.Chart(df2_final).mark_point().encode( x=&#39;Year&#39;, y=&#39;Death_Rate&#39;, color = &#39;Age_Group&#39; ).add_selection( select_country2 ).transform_filter(select_country2) .",
            "url": "https://lucylin1997.github.io/fastpage_copy/jupyter/2021/09/30/Assignment3.html",
            "relUrl": "/jupyter/2021/09/30/Assignment3.html",
            "date": " • Sep 30, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "import altair as alt from vega_datasets import data cars = data.cars.url alt.Chart(cars).mark_point().encode( x=&#39;Miles_per_Gallon:Q&#39;, y=&#39;Horsepower:Q&#39;, color=&#39;Origin:N&#39; ) . import pandas as pd import altair as alt df = pd.read_csv(&#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-11-13/malaria_deaths.csv&#39;) #df.head() df.columns = [&#39;name&#39;, &#39;ISO3166-1-Alpha-3&#39;,&#39;Year&#39;,&#39;Death_Rate&#39;] #df[&#39;Year&#39;] = pd.to_datetime(df[&#39;Year&#39;], format=&#39;%Y&#39;) . country_info = pd.read_csv(&quot;https://raw.githubusercontent.com/datasets/country-codes/master/data/country-codes.csv&quot;,dtype = &#39;str&#39;) . df_new = df[df[&#39;ISO3166-1-Alpha-3&#39;].isin(country_info[&#39;ISO3166-1-Alpha-3&#39;])] df_new.head() . name ISO3166-1-Alpha-3 Year Death_Rate . 0 Afghanistan | AFG | 1990 | 6.802930 | . 1 Afghanistan | AFG | 1991 | 6.973494 | . 2 Afghanistan | AFG | 1992 | 6.989882 | . 3 Afghanistan | AFG | 1993 | 7.088983 | . 4 Afghanistan | AFG | 1994 | 7.392472 | . df_final = pd.merge(df_new, country_info , on = &#39;ISO3166-1-Alpha-3&#39;, how = &#39;left&#39;) . df_final = df_final[[&#39;Year&#39;,&#39;Death_Rate&#39;,&#39;ISO3166-1-numeric&#39;]] df_final.head() . Year Death_Rate ISO3166-1-numeric . 0 1990 | 6.802930 | 004 | . 1 1991 | 6.973494 | 004 | . 2 1992 | 6.989882 | 004 | . 3 1993 | 7.088983 | 004 | . 4 1994 | 7.392472 | 004 | . alt.data_transformers.disable_max_rows() countries = alt.topo_feature(data.world_110m.url, &#39;countries&#39;) slider = alt.binding_range( step=1, min=1990, max=2016 ) select_date = alt.selection_single( name=&quot;slider&quot;, fields=[&#39;Year&#39;], bind=slider, ) . from vega_datasets import data alt.data_transformers.disable_max_rows() countries = alt.topo_feature(data.world_110m.url, &#39;countries&#39;) slider = alt.binding_range( step=1, min=1990, max=2016 ) select_date = alt.selection_single( name=&quot;slider&quot;, fields=[&#39;Year&#39;], bind=slider, ) alt.Chart(df_final).mark_geoshape() .encode(color=&#39;Death_Rate:Q&#39;) .add_selection(select_date) .transform_filter(select_date) .transform_lookup( lookup=&#39;ISO3166-1-numeric&#39;, from_=alt.LookupData(countries, key=&#39;id&#39;, fields=[&quot;type&quot;, &quot;properties&quot;, &quot;geometry&quot;]) ) .project(&#39;equirectangular&#39;) .properties( width=500, height=300, title=&#39;Title&#39; ) .",
            "url": "https://lucylin1997.github.io/fastpage_copy/2021/09/28/Assignment_Part4.html",
            "relUrl": "/2021/09/28/Assignment_Part4.html",
            "date": " • Sep 28, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Title",
            "content": "import pandas as pd import altair as alt df = pd.read_csv(&#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-11-13/malaria_deaths.csv&#39;) #df.head() df.columns = [&#39;name&#39;, &#39;ISO3166-1-Alpha-3&#39;,&#39;Year&#39;,&#39;Death_Rate&#39;] #df[&#39;Year&#39;] = pd.to_datetime(df[&#39;Year&#39;], format=&#39;%Y&#39;) . country_info = pd.read_csv(&quot;https://raw.githubusercontent.com/datasets/country-codes/master/data/country-codes.csv&quot;,dtype = &#39;str&#39;) . df_new = df[df[&#39;ISO3166-1-Alpha-3&#39;].isin(country_info[&#39;ISO3166-1-Alpha-3&#39;])] df_new.head() . name ISO3166-1-Alpha-3 Year Death_Rate . 0 Afghanistan | AFG | 1990 | 6.802930 | . 1 Afghanistan | AFG | 1991 | 6.973494 | . 2 Afghanistan | AFG | 1992 | 6.989882 | . 3 Afghanistan | AFG | 1993 | 7.088983 | . 4 Afghanistan | AFG | 1994 | 7.392472 | . df_final = pd.merge(df_new, country_info , on = &#39;ISO3166-1-Alpha-3&#39;, how = &#39;left&#39;) . df_final = df_final[[&#39;Year&#39;,&#39;Death_Rate&#39;,&#39;ISO3166-1-numeric&#39;]] df_final.head() . Year Death_Rate ISO3166-1-numeric . 0 1990 | 6.802930 | 004 | . 1 1991 | 6.973494 | 004 | . 2 1992 | 6.989882 | 004 | . 3 1993 | 7.088983 | 004 | . 4 1994 | 7.392472 | 004 | . alt.data_transformers.disable_max_rows() countries = alt.topo_feature(data.world_110m.url, &#39;countries&#39;) slider = alt.binding_range( step=1, min=1990, max=2016 ) select_date = alt.selection_single( name=&quot;slider&quot;, fields=[&#39;Year&#39;], bind=slider, ) . from vega_datasets import data alt.data_transformers.disable_max_rows() countries = alt.topo_feature(data.world_110m.url, &#39;countries&#39;) slider = alt.binding_range( step=1, min=1990, max=2016 ) select_date = alt.selection_single( name=&quot;slider&quot;, fields=[&#39;Year&#39;], bind=slider, ) alt.Chart(df_final).mark_geoshape() .encode(color=&#39;Death_Rate:Q&#39;) .add_selection(select_date) .transform_filter(select_date) .transform_lookup( lookup=&#39;ISO3166-1-numeric&#39;, from_=alt.LookupData(countries, key=&#39;id&#39;, fields=[&quot;type&quot;, &quot;properties&quot;, &quot;geometry&quot;]) ) .project(&#39;orthographic&#39;) .properties( width=400, height=300, title=&#39;Malaria Death Rate (per 100,000 people) &#39; ) . df_final[&#39;Year&#39;] = pd.to_datetime(df_final[&#39;Year&#39;], format=&#39;%Y&#39;) country_list = df_final[&#39;ISO3166-1-numeric&#39;].dropna().unique() country_list = country_list.tolist() dropdown = alt.binding_select( options = country_list ) select_country = alt.selection_single( name=&quot;dropdown&quot;, fields=[&#39;ISO3166-1-numeric&#39;], bind = dropdown, ) alt.Chart(df_final).mark_line().encode( x=&#39;Year&#39;, y=&#39;Death_Rate&#39;, ).add_selection( select_country ).transform_filter(select_country) . df1 = pd.read_csv(&#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-11-13/malaria_inc.csv&#39;) df1.columns = [&#39;name&#39;, &#39;ISO3166-1-Alpha-3&#39;,&#39;Year&#39;,&#39;Incidence_Rate&#39;] df1_new = df1[df1[&#39;ISO3166-1-Alpha-3&#39;].isin(country_info[&#39;ISO3166-1-Alpha-3&#39;])] df1_new.head() . name ISO3166-1-Alpha-3 Year Incidence_Rate . 0 Afghanistan | AFG | 2000 | 107.100000 | . 1 Afghanistan | AFG | 2005 | 46.500000 | . 2 Afghanistan | AFG | 2010 | 23.900000 | . 3 Afghanistan | AFG | 2015 | 23.600000 | . 4 Algeria | DZA | 2000 | 0.037746 | . df1_final = pd.merge(df1_new, country_info , on = &#39;ISO3166-1-Alpha-3&#39;, how = &#39;left&#39;) df1_final = df1_final[[&#39;Year&#39;,&#39;Incidence_Rate&#39;,&#39;ISO3166-1-numeric&#39;]] df1_final.head() . Year Incidence_Rate ISO3166-1-numeric . 0 2000 | 107.100000 | 004 | . 1 2005 | 46.500000 | 004 | . 2 2010 | 23.900000 | 004 | . 3 2015 | 23.600000 | 004 | . 4 2000 | 0.037746 | 012 | . alt.data_transformers.disable_max_rows() countries = alt.topo_feature(data.world_110m.url, &#39;countries&#39;) slider1 = alt.binding_range( step=5, min=2000, max=2015 ) select_date1 = alt.selection_single( name=&quot;slider&quot;, fields=[&#39;Year&#39;], bind=slider1, ) . from vega_datasets import data alt.Chart(df1_final).mark_geoshape() .encode(color=&#39;Incidence_Rate:Q&#39;) .add_selection(select_date1) .transform_filter(select_date1) .transform_lookup( lookup=&#39;ISO3166-1-numeric&#39;, from_=alt.LookupData(countries, key=&#39;id&#39;, fields=[&quot;type&quot;, &quot;properties&quot;, &quot;geometry&quot;]) ) .project(&#39;orthographic&#39;) .properties( width=400, height=300, title=&#39;Malaria Incidence Rate (per 1,000 people) &#39; ) . df2 = pd.read_csv(&#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-11-13/malaria_deaths_age.csv&#39;) df2.columns = [&#39;index&#39;,&#39;name&#39;, &#39;ISO3166-1-Alpha-3&#39;,&#39;Year&#39;,&#39;Age_Group&#39;,&#39;Death_Rate&#39;] df2_new = df2[df2[&#39;ISO3166-1-Alpha-3&#39;].isin(country_info[&#39;ISO3166-1-Alpha-3&#39;])] df2_new.head() . index name ISO3166-1-Alpha-3 Year Age_Group Death_Rate . 0 1 | Afghanistan | AFG | 1990 | Under 5 | 184.606435 | . 1 2 | Afghanistan | AFG | 1991 | Under 5 | 191.658193 | . 2 3 | Afghanistan | AFG | 1992 | Under 5 | 197.140197 | . 3 4 | Afghanistan | AFG | 1993 | Under 5 | 207.357753 | . 4 5 | Afghanistan | AFG | 1994 | Under 5 | 226.209363 | . df2_final = pd.merge(df2_new, country_info , on = &#39;ISO3166-1-Alpha-3&#39;, how = &#39;left&#39;) df2_final = df2_final[[&#39;Year&#39;,&#39;Death_Rate&#39;,&#39;ISO3166-1-numeric&#39;,&#39;Age_Group&#39;]] df2_final.head() . Year Death_Rate ISO3166-1-numeric Age_Group . 0 1990 | 184.606435 | 004 | Under 5 | . 1 1991 | 191.658193 | 004 | Under 5 | . 2 1992 | 197.140197 | 004 | Under 5 | . 3 1993 | 207.357753 | 004 | Under 5 | . 4 1994 | 226.209363 | 004 | Under 5 | . df2_final[&#39;Year&#39;] = pd.to_datetime(df2_final[&#39;Year&#39;], format=&#39;%Y&#39;) country_list2 = df2_final[&#39;ISO3166-1-numeric&#39;].dropna().unique() country_list2 = country_list2.tolist() dropdown2 = alt.binding_select( options = country_list2 ) select_country2 = alt.selection_single( name=&quot;dropdown&quot;, fields=[&#39;ISO3166-1-numeric&#39;], bind = dropdown2, ) alt.Chart(df2_final).mark_point().encode( x=&#39;Year&#39;, y=&#39;Death_Rate&#39;, color = &#39;Age_Group&#39; ).add_selection( select_country2 ).transform_filter(select_country2) .",
            "url": "https://lucylin1997.github.io/fastpage_copy/2021/09/28/Assignment3.html",
            "relUrl": "/2021/09/28/Assignment3.html",
            "date": " • Sep 28, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Assignment 2",
            "content": "import numpy as np import pandas as pd from decimal import * import math import unittest from sympy import * . Find the first 10-digit prime in the decimal expansion of $17 pi$ . The first 5 digits in the decimal expansion of $ pi$ are 14159. The first 4-digit prime in the decimal expansion of $ pi$ are 4159. You are asked to find the first 10-digit prime in the decimal expansion of $17 pi$. . Task 1: Write a function to generate an arbitrary large expansion of a mathematical expression like $ pi$. . The 3rd party library sympy has a function called N(expr, &lt;args&gt;) that allows us to directly expand the expression to a certain precision. . &gt;&gt; N(pi, 5) 3.1416 . def generate_expansion(precision, expression): &quot;&quot;&quot; This function returns an expansion of a mathematical expression given the precision. &quot;&quot;&quot; return N(expression, precision) . &gt;&gt; a = generate_expansion(5, pi) &gt;&gt; a 3.1416 &gt;&gt; type(a) sympy.core.numbers.Float . Then we can run a unit test to test the generate_expansion function | . class TestNotebook(unittest.TestCase): def test_genexp(self): self.assertEqual(str(generate_expansion(10, pi)), &#39;3.141592654&#39;) unittest.main(argv=[&#39;&#39;], verbosity=2, exit=False) . test_genexp (__main__.TestFunc) ... ok test_genexp (__main__.TestNotebook) ... ok - Ran 2 tests in 0.002s OK . &lt;unittest.main.TestProgram at 0x23203530ca0&gt; . Task 2: Write a function to check if a number is a prime number . Based on the definition of prime number, we need to check whether the number can be divided by any number that is less than the square root of the certain number. | . def is_prime(num): &quot;&quot;&quot; The function returns whether a given number is prime or not &quot;&quot;&quot; if num &gt; 1: for i in range(2, int(math.sqrt(num))+1): if (num % i) == 0: return False #break return True . &gt;&gt; is_prime(40) False . Then we can run a unit test to test the is_prime function | . class TestNotebook(unittest.TestCase): def test_isprime(self): self.assertEqual(is_prime(29), True) # 29 is a prime number self.assertEqual(is_prime(30), False) # 30 is not a prime number unittest.main(argv=[&#39;&#39;], verbosity=2, exit=False) . test_genexp (__main__.TestFunc) ... ok test_isprime (__main__.TestNotebook) ... ok - Ran 2 tests in 0.005s OK . &lt;unittest.main.TestProgram at 0x2320352b910&gt; . Task 3: Slicing Window . def slicing_window (number_str, idx): &quot;&quot;&quot; The function returns the specified width from a long iterable, the inputs are the string format of a number and the start point of the number &quot;&quot;&quot; return int(number_str[idx: idx+10]) . The slicing_window works like below . &gt;&gt; slicing_window(&#39;12345678899443878169846&#39;,1) 2345678899 . Then we can run a unit test to test the slicing_window function | . class TestNotebook(unittest.TestCase): def test_slicingwindow(self): self.assertEqual(slicing_window(&#39;12345667788990&#39;,2), 3456677889) unittest.main(argv=[&#39;&#39;], verbosity=2, exit=False) . test_genexp (__main__.TestFunc) ... ok test_slicingwindow (__main__.TestNotebook) ... ok - Ran 2 tests in 0.003s OK . &lt;unittest.main.TestProgram at 0x23203542760&gt; . def find_prime(precision, expression): &quot;&quot;&quot;The function returns the first 10-digit number in the expansion of the expression &quot;&quot;&quot; expansion = generate_expansion(precision, expression) formula = str(expansion) # The output of the generate_expansion is float, we need to convert it to string first string = formula.replace(&quot;.&quot;, &quot;&quot;) # replace the decimal point in the expansion #initial = False for idx in range(len(string)): if is_prime(slicing_window(string, idx)) is True: print( f&#39;The first 10-digit prime of {expression} is {slicing_window(string, idx)} and the start point is at {idx}&#39;) return slicing_window(string, idx) break . Then we can run a unit test to test the find_prime function | . class TestNotebook(unittest.TestCase): def test_slicingwindow(self): self.assertEqual(find_prime(500, exp(1)), 7427466391) unittest.main(argv=[&#39;&#39;], verbosity=2, exit=False) . test_genexp (__main__.TestFunc) ... ok test_slicingwindow (__main__.TestNotebook) ... . The first 10-digit prime of E is 7427466391 and the start point is at 99 . ok - Ran 2 tests in 0.015s OK . &lt;unittest.main.TestProgram at 0x23203542b20&gt; . The the final solution to the problem is: | . find_prime(500, 17*pi) . The first 10-digit prime of 17*pi is 8649375157 and the start point is at 20 . 8649375157 .",
            "url": "https://lucylin1997.github.io/fastpage_copy/jupyter/2021/09/17/Assignment2_Yili-Lin.html",
            "relUrl": "/jupyter/2021/09/17/Assignment2_Yili-Lin.html",
            "date": " • Sep 17, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Assignment 1",
            "content": "Question 1 Problem 15 Lattice Path . Starting in the top left corner of a 2×2 grid, and only being able to move to the right and down, there are exactly 6 routes to the bottom right corner. . . How many such routes are there through a 20×20 grid? . Solution to Question 1 . We can first consider the grid as a matrix, each element in the matrix represents the number of routes to that a location in the grid. For example, for a 2×2 grid, the corresponding matrix ${a_{i,j}}$ looks like this: $$ left( begin{array}{l} 0&amp; 1 &amp; 1 1&amp; 2&amp;3 1&amp; 3 &amp;6 end{array} right) $$ $a_{2,2}=6$, meaning that there are 6 routes to the bottom right corner. From the $2 times 2$ grid example and our observation, we find that there is always only one route for those located on the left and top of the grid. And for the remaining locations, the number of routes depend on those located on its top and left. So it is a dynamic programming problem. . import numpy as np def count_path(n): &quot;&quot;&quot;The parameter of the function is the dimension of the grid and it will return the number of routes to the bottom right corner of the grid &quot;&quot;&quot; # Dynamic Programming # Initilize the condition boundary mat = np.zeros((n+1, n+1)) for idx in range(len(mat)): mat[0][idx] = 1 # The top boundary of the grid only has one routes mat[idx][0] = 1 # The left boundary of the grid has only one routes for row in range(1, len(mat)): #dynamic programming for col in range(1, len(mat)): mat[row, col] = mat[row, col-1] + mat[row-1, col] return mat[n][n] #return the location on the bottom right corner if __name__ == &#39;__main__&#39;: print(count_path(20)) . 137846528820.0 . Question 2 Problem 50 Distinct Prime Factors . The prime 41, can be written as the sum of six consecutive primes: 41 = 2 + 3 + 5 + 7 + 11 + 13 This is the longest sum of consecutive primes that adds to a prime below one-hundred. . The longest sum of consecutive primes below one-thousand that adds to a prime, contains 21 terms, and is equal to 953. . Which prime, below one-million, can be written as the sum of the most consecutive primes? . Solution to Question 2 . First, we need to generate a list of prime number. There is a traditional algorithm called sieve of eratosthenes, which is designed for generating prime numbers less than a given value. . Sieve of Eratosthenes . Create a list of n-1 consecutive integers 2, 3, 4,..., n. 2 is the smallest prime number | For the first loop, set k to 2 | Let k increase k every step until it reaches n, that is, the algorithm loop over k, 2k, 3k,.. and mark these numbers in the list | Then find the smallest number in the list that is not marked and if no such number exists, the algorithm will stop. Otherwise, set k to the new number and iterate again. | At the end of the algorithm, all the numbers that are not marked are the prime numbers.Example:Find prime less than 20 &gt; List all the number that is less than 20 . 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20 . Start with 2, we will mark 2, 4, 6,.. 20 . 2, 3, 5, 7, 9, 11, 13, 15, 17, 19 . The next start number is 3, we will mark 9,15 . 2, 3, 5, 7, 11, 13, 17, 19 . Then we can see that all the prime numbers less than 20 are listed in the above. . | With the sieve of eratosthenes algorithm, we can find all the prime numbers that are less than the given limit. When the limit is 100, the prime list is [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]. For the first loop, We start with the first prime number 2, and we will loop over the whole prime list to check if 2, 2 + 3, 2 + 3 + 5... is a prime number and if the summation exceeds 100. When the summation is a prime number, the current length of the consective prime numbers is the difference between the index of the first number and the last number, and if the summation exceeds 100, the stop index is the index of the last number. In this case, 2 + 3 + 5 + 7+ 11 + 13 + 17 + 19 + 23 = 100, so the stop index should be 9. Since 2 + 3 + 5 + 7+ 11 + 13= 41 is the longest sum of consecutive prime for the first loop, the length of consecutive after first loop is 6. Similarly we start with prime number 3, 5,.... until the algorithm stops. . import numpy as np import math def sieve_func(n): bool_list = [True] * n #This is the list used to indicate whether the number is marked, initially set all the elements to True bool_list[0] = False # 0 is not a prime number bool_list[1] = False # 1 is not a prime number for i in range(2, int(math.sqrt(n))+1): if bool_list[i] is True: for j in range(i**2, n, i): bool_list[j] = False prime_list = [index for index, boolvalue in enumerate(bool_list) if boolvalue == True] # find all the numbers that are not marked return prime_list . def find_distinct_factor(limit): &quot;&quot;&quot; The parameter of this function is the limit value, and the function will return the largest prime that can be written as the sum of the most consecutive primes under the given limit &quot;&quot;&quot; primes = sieve_func(limit) # Set the original length of the consecutive number to be 0 raw_length = 0 # initial value of the consecutive prime sum largest_sum = 0 # the last index for the end of the consecutive number, that is, if the index of the prime in the list exceeds this number, the summation will be larger than the given value # Initially set to the length of the prime list last_idx = len(primes) # two for loops for i in range(len(primes)): for j in range(i + raw_length, last_idx): summation = sum(primes[i:j]) if summation &lt; limit: if summation in primes: raw_length = j - i largest_sum = summation else: # When the summation of the consecutive number exceeds 100, we will set the stop point last_idx = j + 1 break return largest_sum . print(find_distinct_factor(1000000)) . 997651 . Question 3 Problem 123 Prime square remainders . Let $p_n$ be the nth prime: 2, 3, 5, 7, 11,... and let r be the reminder when $(p_n-1)^n + (p_n+1)^n$ is divided by $p_n^2$. . For example, when n = 3, $p_3 =5$, and $4^3 + 6^3 = 280 = 5 mod 25$. . The least value of n for which the reminder first exceeds $10^9$ is 7037. . Find the least value of n for which the reminder first exceeds $10^{10}$. . Solution to Question 3 . The expansion of $(p_n-1)^n$: . $(p_n-1)^n = p_n^n - (n-1)p_n^{n-1}+ binom{n}{n-2}p_n^{n-2}+...+(-1)^{n-1}np_n+(-1)^n$. . The expansion of $(p_n+1)^n$ is: . $(p_n+1)^n= p_n^n+ (n-1)p_n^{n-1}+ binom{n}{n-2}p_n^{n-2}+...+np_n+ 1$. . From the equation above we know that except for the last two terms, all the other terms are divisable by $p_n^2$. Thus, the reminder when $(p_n-1)^n+(p_n+1)^n$ is divided by $p_n^2$ should be: . $r = (-1)^{n-1}np_n+(-1)^n +np_n+1$. . So when n is an odd value, $r = 2np_n$, when n is an even number, $r= 2$. The question requires us to find the least value of n for which the reminder first exceeds $10^{10}$, so n must be an odd number. As is mentioned in the problem description: &quot;the least value of n for which the remainder first exceeds $10^9$ is 7037&quot;, thus the start point can begin with 7037 and we only care about the odd number. We can use the sieve function we defined in Question 2 and list all the prime number which is less than 5000000 (This number does not need to be exact 5000000, we just need to define a large number to make sure that there are enough prime numbers). Then we can loop over the prime number and find the least value of n when the reminder exceeds $10^{10}$ . def find_reminder(n): &quot;&quot;&quot; The find_reminder() function has one parameter n, which is the limit. The function will return the least value of n when the reminder exceeds the given limit. &quot;&quot;&quot; initial_n = 7037 #Set the initial value of n to 7037 limit = n #max limit for the reminder product = 0 primeslist = sieve_func(5_000_000) # Find all the prime number which is less than 5_000_000, the limit can vary while product &lt;= limit: initial_n += 2 product = 2 * initial_n * primeslist[initial_n-1] return initial_n if __name__ == &#39;__main__&#39;: print(find_reminder(10000000000)) . 21035 .",
            "url": "https://lucylin1997.github.io/fastpage_copy/jupyter/2021/09/01/Assignment1.html",
            "relUrl": "/jupyter/2021/09/01/Assignment1.html",
            "date": " • Sep 1, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Profile . Lucy Lin, a devoted and meticulous Master of Biostatistics candidate from Duke University with a solid background in mathematics and statistics, enthusiastic about the application of electronic health data in providing better health care to patients. Currently seeking an internship in Biostatistics/Data Science. Education . Duke University, School of Medicine . Master of Biostatistics &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Aug,2020 - May, 2022 . Relevant courses: Introduction to Statistical Theory and Methods I and II, Applied Biostatistics Methods I and II, Introduction to Practice of Biostatistics I and II, Introduction to Statistical Programming I and II, Software Tools for Data Science. . Shanghai University of Finance and Economics . Information and Computing Science &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Sep,2016 - Jun,2020 . Revelent courses: Data Structure, Big Data Process, Statistical Computing, Time series Analysis, Introduction of Stochastic Processes, Differential Equations. .",
          "url": "https://lucylin1997.github.io/fastpage_copy/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://lucylin1997.github.io/fastpage_copy/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}